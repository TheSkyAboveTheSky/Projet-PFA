{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPi4aLjZPwqKn22B/O2+bQh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheSkyAboveTheSky/Projet-PFA/blob/main/PFA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import**"
      ],
      "metadata": {
        "id": "6_phIWyJAl72"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "bUQOuCko-R_5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data**"
      ],
      "metadata": {
        "id": "jFasUVCbAzi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def removeImages(data_dir) :\n",
        "  # List all the images in the folder\n",
        "  file_list = os.listdir(data_dir)\n",
        "  # Remove them all\n",
        "  for file_name in file_list:\n",
        "      file_path = os.path.join(data_dir, file_name)\n",
        "      if os.path.isfile(file_path):\n",
        "          os.remove(file_path)\n",
        "\n",
        "# Data folder path\n",
        "data_dir = Path(\"./data/\")\n",
        "# Get list of all the images\n",
        "images = sorted(list(map(str, list(data_dir.glob(\"*.png\")))), key=lambda x: int(''.join(filter(str.isdigit, os.path.splitext(os.path.basename(x))[0]))))\n",
        "# Read labels from csv file\n",
        "labels = []\n",
        "with open('label.csv','r') as f:\n",
        "  reader = csv.reader(f)\n",
        "  # skip the columns name\n",
        "  next(reader)\n",
        "  for row in reader:\n",
        "    labels.extend([row])\n",
        "# Get the list of all unique characters in the labels\n",
        "characters = sorted(list(set(char for arr in labels for string in arr for char in string)))\n",
        "print(\"Number of images found:\", len(images))\n",
        "print(\"Number of labels found:\", len(labels))\n",
        "print(\"Number of unique characters:\", len(characters))\n",
        "print(\"Characters present:\", characters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBTxTDs3A4Jj",
        "outputId": "bba03365-141b-4f73-cffc-5e32d66c0cd8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images found: 10\n",
            "Number of labels found: 115\n",
            "Number of unique characters: 64\n",
            "Characters present: [' ', '#', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'V', 'W', 'Y', 'Z', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define Our Variable**"
      ],
      "metadata": {
        "id": "uPl8aeNXKRYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_width = 600\n",
        "img_height = 200\n",
        "max_length = 50\n",
        "batch_size = 1\n",
        "input_shape = (img_height,img_width,3)\n",
        "num_classes = len(characters) +1"
      ],
      "metadata": {
        "id": "D-wZgm8LKVGU"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRansform characters into numbers and numbers into characters**"
      ],
      "metadata": {
        "id": "GX345LP7LAYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_num = keras.layers.StringLookup(\n",
        "    vocabulary=characters, mask_token=None\n",
        ")\n",
        "num_to_char = keras.layers.StringLookup(\n",
        "    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",
        ")"
      ],
      "metadata": {
        "id": "hJK_Y90ILA-R"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split data "
      ],
      "metadata": {
        "id": "j9EbwiI6L-qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(images, labels, train_size, shuffle=True):\n",
        "    size = tf.shape(images)[0]\n",
        "    indices = tf.range(size)\n",
        "    if shuffle:\n",
        "        indices = tf.random.shuffle(indices)\n",
        "    train_samples = tf.cast(tf.cast(size, tf.float32) * train_size, tf.int32)\n",
        "    x_train, y_train = tf.gather(images, indices[:train_samples]), tf.gather(labels, indices[:train_samples])\n",
        "    \n",
        "    valid_samples = size - train_samples\n",
        "    \n",
        "    if valid_samples > 0:\n",
        "        x_valid, y_valid = tf.gather(images, indices[train_samples:train_samples+valid_samples]), tf.gather(labels, indices[train_samples:train_samples+valid_samples])\n",
        "    else:\n",
        "        x_valid, y_valid = None, None\n",
        "    \n",
        "    return x_train, x_valid, y_train, y_valid\n",
        "x_train, x_valid, y_train, y_valid = split_data(images, labels,0.99)"
      ],
      "metadata": {
        "id": "oAUp81bdMSBm"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create Datasets**"
      ],
      "metadata": {
        "id": "aNU9lAb4PL5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_single_sample(img_path, label):\n",
        "    # read the image with the tensorflow\n",
        "    img = tf.io.read_file(img_path)\n",
        "    # decode the png image with color\n",
        "    img = tf.image.decode_png(img, channels=3)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    # resizing the image\n",
        "    img = tf.image.resize(img, [img_height, img_width])\n",
        "    # transform the label\n",
        "    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
        "    label = tf.reshape(label, shape=(-1,))\n",
        "    label = tf.ensure_shape(label, (None,))\n",
        "    return {\"image\": img, \"label\": label}\n",
        "\n",
        "# create the datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = (\n",
        "    train_dataset.map(\n",
        "        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "    )\n",
        "    # batch the dataset of batches of our batch_size\n",
        "    .batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
        "validation_dataset = (\n",
        "    validation_dataset.map(\n",
        "        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "    )\n",
        "    .batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        ")\n"
      ],
      "metadata": {
        "id": "-D-bPg4xPKnE"
      },
      "execution_count": 48,
      "outputs": []
    }
  ]
}